---
title: "Omar Rivasplata"
author: ["Omar Rivasplata"]
lastmod: 2023-11-10T21:10:00+00:00
draft: false
weight: 3002
active: true
superuser: false
role: "Incoming Senior Lecturer in Machine Learning"
organizations:
  - name: "University of Manchester"
    url: "https://www.manchester.ac.uk/"
  - name: "Personal Webpage"
    url: "https://www.homepages.ucl.ac.uk/~ucabriv/"

# Interests to show in About widget
interests:
  - Machine Learning
  - Statistical Learning Theory
  - Risk bounds, PAC-Bayes
  - Optimisation & Certification
  - Probabilistic Methods

  
# Highlight the author in author lists? (true/false)
highlight_name: false

# Organizational groups that you belong to (for People widget)
#   Remove this if you are not using the People widget.
user_groups:
- Supervisor
---

I am an incoming Senior Lecturer (Associate Professor) in Machine Learning in the Department of Computer Science at the University of Manchester, where I will be a member of the [Manchester Centre for AI Fundamentals](https://ai-fun.manchester.ac.uk/). 

For the time being I continue to be a Senior Research Fellow in the Department of Statistical Science at University College London, where I lead the research group [DELTA](https://www.homepages.ucl.ac.uk/~ucabriv/delta.html).

My work is on machine learning research. This field is fascinating! One of the things I enjoy most about it being the confluence of maths and stats, and computer science experiments, to answer research questions. Besides statistical learning I am interested also in other learning frameworks such as online learning and reinforcement learning, and of course deep learning, which is quite popular these days. It looks that optimisation is one pervasive theme across machine learning theory and practice, though it comes up in such a variety of flavours and colours that it isn't boring. It reminds of the [least action principle](https://en.wikipedia.org/wiki/Stationary-action_principle) of [Maupertuis](https://en.wikipedia.org/wiki/Pierre_Louis_Maupertuis), saying that "everything happens as if some quantity was to be made as small as possible." (This principle has lead the optimists to believe that we live in [the best possible world](https://www.google.co.uk/books/edition/_/WGOmFLikLrkC?hl=en).) But just optimisation doesn't quite do it for machine learning... to really be talking about learning one has to pay attention to generalisation!

